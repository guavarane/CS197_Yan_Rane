# -*- coding: utf-8 -*-
"""197-ProgressReport1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X2DDHtu3Z6b_wVGgG_5EIfC_tA_lfiUG
"""

import os
import pickle
import random
import math

!pip install rdkit
!pip install selfies
import rdkit
import selfies

file = open('ESM_embedding.pickle', 'rb')

#Hey Gaurav, this doesn't work for me because of the difference in ID numbers, would you be able to share the file with me as well?
from google.colab import drive
drive.mount('/content/drive')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id='1m0DpYdE_oDozjitO6N-4V5O3ATTRpPGf'
downloaded = drive.CreateFile({'id': id}) 
downloaded.GetContentFile('DATASET.csv')

id='1WlFx9yA3OUAcNL_US1gijhbywwPn2IS2'
downloaded = drive.CreateFile({'id': id}) 
downloaded.GetContentFile('ESM_embedding.pickle')

# Data Loading Method
with open('DATASET.csv') as f:
    dataset_lines = f.readlines()

with open('ESM_embedding.pickle', 'rb') as input_file:
    esm_embeddings_all = pickle.load(input_file)

# Small Molecule Encoding
A = dataset_lines[0].split(',')
smile_string = A[0]
protein_sequence = A[1].strip()
esm_emb_prot = esm_embeddings_all[0]
binding_mes = float(A[2].strip())
# Encode it as a selfies string, we chose this because it has a representation of the molecule
# that is understandable structurally and textually. It seems like it is able to represent a lot
# of information just as a selfies string, so we went with that.
self_str = selfies.encoder(smile_string)

print('Smile String: ' + smile_string)
print('Selfies String: ' + self_str)
print('Protein Sequence: ' + protein_sequence)
print('ESM Embedding Protein: ' + str(esm_emb_prot))
print('Binding Measurement: ' + str(binding_mes))

random.shuffle(dataset_lines)
print(len(dataset_lines))
training_set = dataset_lines[:math.ceil((len(dataset_lines) * 0.8))]
test_set = dataset_lines[math.ceil((len(dataset_lines) * 0.8)):]

!pip install fair-esm
import torch
import esm 
import time

start_time = time.time()

# Load ESM-2 model
model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()
batch_converter = alphabet.get_batch_converter()
model.eval()  # disables dropout for deterministic results


data = [
    ("protein1", "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG"),
    ("protein2", "AALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEEKALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE")
]
batch_labels, batch_strs, batch_tokens = batch_converter(data)
batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)

# Extract per-residue representations (on CPU)
with torch.no_grad():
    results = model(batch_tokens, repr_layers=[33], return_contacts=True)
token_representations = results["representations"][33]

print('Total time: ', time.time()-start_time)
token_representations.shape