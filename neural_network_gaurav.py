# -*- coding: utf-8 -*-
"""Neural Network Gaurav.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O49n7vx1MbY-qFUAGO4ILsB04KRQ0nPI
"""

!pip install rdkit
!pip install selfies

import os
import pickle
import rdkit
import selfies
import csv
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem

from google.colab import drive
drive.mount('/content/drive')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id='1B9Jsi8JLzn8a1N7jOLzwu4hGiI9nwYtf'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('DATASET.csv')

id='1xzVjnfVXp-QmGz5OB40r-nCtHh1uPlNx'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('ESM_embedding.pickle')

# def generate_master_data():
#     with open('DATASET.csv') as f:
#         dataset_lines = f.readlines()

#     with open('ESM_embedding.pickle', 'rb') as input_file:
#         esm_embeddings_all = pickle.load(input_file)

#     master_data = []

#     for i, line in enumerate(dataset_lines):
#         if i % 5000 == 0:
#             print('On:{}/{}'.format( i, len(dataset_lines)))
#         A = line.split(',')
#         smile_string = A[0]
#         k_d = A[2].strip()
#         if k_d[0] == '>' or k_d[0] == '<':
#             binding_mes = float(k_d[1:])
#         else:
#             binding_mes = float(k_d)
#         esm_emb_prot = esm_embeddings_all[i]

#         mol = Chem.MolFromSmiles(smile_string)
#         bit_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)
#         bit_vec = np.array(bit_vec)
#         bit_vec = list(bit_vec)
#         esm_emb_prot = np.array(esm_emb_prot)
#         esm_emb_prot = list(esm_emb_prot)
#         bit_vec.extend(esm_emb_prot)
#         bit_vec.append(binding_mes)
#         master_data.append(bit_vec)



#     return np.array(master_data)

def generate_master_data():
    with open('DATASET.csv') as f:
        dataset_lines = f.readlines()

    with open('ESM_embedding.pickle', 'rb') as input_file:
        esm_embeddings_all = pickle.load(input_file)

    master_data = []
    binding_mes_values = []

    for i, line in enumerate(dataset_lines):
        if i % 5000 == 0:
            print('On:{}/{}'.format( i, len(dataset_lines)))
        A = line.split(',')
        smile_string = A[0]
        k_d = A[2].strip()
        if k_d[0] == '>' or k_d[0] == '<':
            binding_mes = float(k_d[1:])
        else:
            binding_mes = float(k_d)
        binding_mes_values.append(binding_mes)
        esm_emb_prot = esm_embeddings_all[i]

        mol = Chem.MolFromSmiles(smile_string)
        bit_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)
        bit_vec = np.array(bit_vec)
        bit_vec = list(bit_vec)
        esm_emb_prot = np.array(esm_emb_prot)
        esm_emb_prot = list(esm_emb_prot)
        bit_vec.extend(esm_emb_prot)
        bit_vec.append(binding_mes)
        master_data.append(bit_vec)
        if i == 0:
          print(bit_vec)

    binding_min = min(binding_mes_values)
    binding_max = max(binding_mes_values)

    for i in range(len(master_data)):
        master_data[i][-1] = (master_data[i][-1] - binding_min) / (binding_max - binding_min)

    return np.array(master_data)

import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.data import random_split


class KD_Predictor(nn.Module):
    def __init__(self, input_dim):
        super(KD_Predictor, self).__init__()
        self.linear1 = nn.Linear(input_dim, 1000)
        self.bn1 = nn.BatchNorm1d(1000)
        self.relu = nn.LeakyReLU()
        self.dropout = nn.Dropout(0.5)
        # self.relu = nn.ReLU()

        self.linear2 = nn.Linear(1000, 500)
        self.bn2 = nn.BatchNorm1d(500)

        self.linear3 = nn.Linear(500, 1)

    def forward(self, x):
        x = self.linear1(x)
        x = self.bn1(x)
        x = self.relu(x)
        # x = self.dropout(x)

        x = self.linear2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.dropout(x)

        x = self.linear3(x)

        return x

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# Loading and preparing the data
master_data = generate_master_data()

X = master_data[:, :-1]  # feature vectors
Y = master_data[:, -1]  # k_d values


# Converting numpy arrays to PyTorch tensors
X = torch.tensor(X, dtype=torch.float32).to(device)
Y = torch.tensor(Y, dtype=torch.float32).view(-1, 1).to(device)

# Create a TensorDataset and a DataLoader
dataset = TensorDataset(X, Y)

# TODO: segment out the dataset into 3 chunks
dataset_size = len(dataset)
train_size = int(0.7 * dataset_size)
val_size = int(0.15 * dataset_size)
test_size = dataset_size - train_size - val_size
print(train_size, val_size, test_size, dataset_size)

# Split the dataset
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Create data loaders for each set
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)
# Train, Val, and Test

# dataloader = DataLoader(dataset, batch_size=256, shuffle=True)

# Instantiate the model
input_dim = X.shape[1]  # Number of input features
model = KD_Predictor(input_dim).to(device)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# # Training Loop
# best_val_loss = float('inf')

# for epoch in range(50):
#     model.train()
#     losses = []  # list to store all losses per epoch
#     for batch_X, batch_Y in train_loader:
#         batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)

#         optimizer.zero_grad()
#         outputs = model(batch_X)
#         loss = criterion(outputs, batch_Y)
#         loss.backward()
#         optimizer.step()
#         losses.append(loss.item())

#     val_losses = []
#     model.eval()
#     with torch.no_grad():
#         for batch_X, batch_Y in val_loader:
#             batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)
#             outputs = model(batch_X)
#             loss = criterion(outputs, batch_Y)
#             val_losses.append(loss.item())

#     avg_val_loss = sum(val_losses) / len(val_losses)
#     print('Epoch: {}, Validation Loss: {}'.format(epoch, avg_val_loss))

#     # Save the model if validation loss decreased
#     if avg_val_loss < best_val_loss:
#         best_val_loss = avg_val_loss
#         torch.save(model.state_dict(), 'best_model.pth')

best_val_loss = float('inf')

scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

for epoch in range(50):
    model.train()
    losses = []  # list to store all losses per epoch
    for batch_X, batch_Y in train_loader:
        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_Y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    avg_train_loss = sum(losses) / len(losses)
    print('Epoch: {}, Training Loss: {}'.format(epoch, avg_train_loss))

    val_losses = []
    model.eval()
    with torch.no_grad():
        for batch_X, batch_Y in val_loader:
            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)
            outputs = model(batch_X)
            loss = criterion(outputs, batch_Y)
            val_losses.append(loss.item())

    avg_val_loss = sum(val_losses) / len(val_losses)
    print('Epoch: {}, Validation Loss: {}'.format(epoch, avg_val_loss))

    # Decay Learning Rate
    scheduler.step()

    # Save the model if validation loss decreased
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), 'best_model.pth')

from scipy.stats import pearsonr

# Load the best model
model = KD_Predictor(input_dim)
model.load_state_dict(torch.load('best_model.pth'))
model.to(device)
model.eval()

predictions = []
actual_values = []

with torch.no_grad():
    for batch_X, batch_Y in test_loader:
        batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)

        outputs = model(batch_X)
        predictions.extend(outputs.detach().cpu().numpy())
        actual_values.extend(batch_Y.detach().cpu().numpy())

predictions = np.array(predictions).flatten()
actual_values = np.array(actual_values).flatten()

r, _ = pearsonr(predictions, actual_values)
print('Pearson r^2: ', r**2)

# Learning Rate Data (w/ Decay)

# LR = 0.01, P = 0.15530980651861404, 0.2780041083341163
# LR = 0.001, P = 0.4456156830198879, 0.4068820146442912
# LR = 0.0001, P = 0.06446679398328509, 0.011517205655965979,

# Learning Rate Data (no decay)

# LR = 0.01, P = 0.0017043034233220052, 0.18946402871930595
# LR = 0.001, P = 0.5112630610927259, 0.37876200634622037
# LR = 0.0001, P = 0.166619460793246, 0.09127284344950833

# Batch Normalization Data

# No Batch, P = 0.4086709469269358, 0.4478133508640979, 0.27083027180019165
# Batch 1st, P = 0.20263851386410775, 0.16016004548875307, 0.18628311026173675
# Batch 2nd, P = 0.3606805587040879, 0.28864843352615216, 0.21199929499138004
# Batch both, P = 0.48030962519294657, 0.43905058028686667, 0.31658071679528255

# Final Test

# 0.5881574842049135, 0.5918855268433499, 0.3539617990041219